{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Classical ML Models for Mindfulness Prediction\n",
                "\n",
                "**Phase 2: Days 5-10**\n",
                "\n",
                "This notebook implements and compares three classical machine learning models:\n",
                "1. **Linear Regression with Regularization** (Ridge, Lasso, ElasticNet)\n",
                "2. **Random Forest Regressor**\n",
                "3. **XGBoost Regressor**\n",
                "\n",
                "Each model includes:\n",
                "- Hyperparameter tuning with GridSearchCV\n",
                "- Comprehensive evaluation (MSE, MAE, R¬≤, RMSE)\n",
                "- Visualizations (actual vs predicted, residuals, feature importance)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import sys\n",
                "import warnings\n",
                "from pathlib import Path\n",
                "\n",
                "# Add src to path\n",
                "sys.path.append('../src')\n",
                "\n",
                "# Import custom modules\n",
                "from models import (\n",
                "    LinearRegressionModel, \n",
                "    RandomForestModel, \n",
                "    XGBoostModel,\n",
                "    compare_models,\n",
                "    plot_actual_vs_predicted,\n",
                "    plot_residuals,\n",
                "    plot_residual_distribution,\n",
                "    plot_feature_importance,\n",
                "    plot_model_comparison\n",
                ")\n",
                "\n",
                "# Configure plotting\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "plt.rcParams['font.size'] = 10\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "RANDOM_STATE = 42\n",
                "np.random.seed(RANDOM_STATE)\n",
                "\n",
                "print(\"‚úì Imports successful\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Preprocessed Data\n",
                "\n",
                "Load the train/test splits created in the preprocessing notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define data paths\n",
                "data_dir = Path('../data/processed')\n",
                "\n",
                "# Load training data\n",
                "X_train = pd.read_csv(data_dir / 'S2_X_train.csv')\n",
                "y_train = pd.read_csv(data_dir / 'S2_y_train.csv').values.ravel()\n",
                "\n",
                "# Load test data\n",
                "X_test = pd.read_csv(data_dir / 'S2_X_test.csv')\n",
                "y_test = pd.read_csv(data_dir / 'S2_y_test.csv').values.ravel()\n",
                "\n",
                "# Get feature names\n",
                "feature_names = X_train.columns.tolist()\n",
                "\n",
                "print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
                "print(f\"Test set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
                "print(f\"\\nTarget distribution (train):\")\n",
                "print(pd.Series(y_train).describe())\n",
                "print(f\"\\nTarget distribution (test):\")\n",
                "print(pd.Series(y_test).describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Baseline Model: Simple Linear Regression\n",
                "\n",
                "Establish baseline performance with unregularized linear regression."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
                "\n",
                "# Train baseline model\n",
                "baseline_model = LinearRegression()\n",
                "baseline_model.fit(X_train, y_train)\n",
                "\n",
                "# Evaluate on test set\n",
                "y_pred_baseline = baseline_model.predict(X_test)\n",
                "\n",
                "baseline_metrics = {\n",
                "    'MSE': mean_squared_error(y_test, y_pred_baseline),\n",
                "    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_baseline)),\n",
                "    'MAE': mean_absolute_error(y_test, y_pred_baseline),\n",
                "    'R2': r2_score(y_test, y_pred_baseline)\n",
                "}\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"BASELINE MODEL: Simple Linear Regression (No Regularization)\")\n",
                "print(\"=\"*60)\n",
                "print(f\"R¬≤ Score:  {baseline_metrics['R2']:.4f}\")\n",
                "print(f\"RMSE:      {baseline_metrics['RMSE']:.4f}\")\n",
                "print(f\"MAE:       {baseline_metrics['MAE']:.4f}\")\n",
                "print(f\"MSE:       {baseline_metrics['MSE']:.4f}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Model 1: Linear Regression with Regularization\n",
                "\n",
                "Train and compare Ridge, Lasso, and ElasticNet regularization."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1 Ridge Regression (L2 Regularization)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize and train Ridge model\n",
                "ridge_model = LinearRegressionModel(model_type='ridge', random_state=RANDOM_STATE)\n",
                "\n",
                "ridge_model.train(\n",
                "    X_train.values, \n",
                "    y_train, \n",
                "    feature_names=feature_names,\n",
                "    tune_hyperparams=True,\n",
                "    cv_folds=5,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "# Evaluate on test set\n",
                "ridge_metrics = ridge_model.evaluate(X_test.values, y_test, dataset_name=\"Test\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Lasso Regression (L1 Regularization)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize and train Lasso model\n",
                "lasso_model = LinearRegressionModel(model_type='lasso', random_state=RANDOM_STATE)\n",
                "\n",
                "lasso_model.train(\n",
                "    X_train.values, \n",
                "    y_train, \n",
                "    feature_names=feature_names,\n",
                "    tune_hyperparams=True,\n",
                "    cv_folds=5,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "# Evaluate on test set\n",
                "lasso_metrics = lasso_model.evaluate(X_test.values, y_test, dataset_name=\"Test\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3 ElasticNet (L1 + L2 Regularization)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize and train ElasticNet model\n",
                "elasticnet_model = LinearRegressionModel(model_type='elasticnet', random_state=RANDOM_STATE)\n",
                "\n",
                "elasticnet_model.train(\n",
                "    X_train.values, \n",
                "    y_train, \n",
                "    feature_names=feature_names,\n",
                "    tune_hyperparams=True,\n",
                "    cv_folds=5,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "# Evaluate on test set\n",
                "elasticnet_metrics = elasticnet_model.evaluate(X_test.values, y_test, dataset_name=\"Test\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.4 Compare Linear Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare linear models\n",
                "linear_models = [ridge_model, lasso_model, elasticnet_model]\n",
                "linear_comparison = compare_models(linear_models, X_test.values, y_test)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"LINEAR MODELS COMPARISON\")\n",
                "print(\"=\"*80)\n",
                "print(linear_comparison.to_string(index=False))\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Select best linear model\n",
                "best_linear_idx = linear_comparison['R¬≤'].idxmax()\n",
                "best_linear_model = linear_models[best_linear_idx]\n",
                "print(f\"\\n‚úì Best Linear Model: {best_linear_model.name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.5 Visualize Best Linear Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Actual vs Predicted\n",
                "y_pred_linear = best_linear_model.predict(X_test.values)\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "# Plot 1: Actual vs Predicted\n",
                "plot_actual_vs_predicted(y_test, y_pred_linear, best_linear_model.name, ax=axes[0])\n",
                "\n",
                "# Plot 2: Residuals\n",
                "plot_residuals(y_test, y_pred_linear, best_linear_model.name, ax=axes[1])\n",
                "\n",
                "# Plot 3: Residual Distribution\n",
                "plot_residual_distribution(y_test, y_pred_linear, best_linear_model.name, ax=axes[2])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.6 Feature Importance (Coefficient Magnitudes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature importance\n",
                "linear_importance = best_linear_model.get_feature_importance()\n",
                "\n",
                "print(\"\\nTop 20 Most Important Features (by coefficient magnitude):\")\n",
                "print(linear_importance.head(20).to_string(index=False))\n",
                "\n",
                "# Plot feature importance\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "plot_feature_importance(linear_importance, top_n=20, model_name=best_linear_model.name, ax=ax)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Model 2: Random Forest Regressor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize and train Random Forest model\n",
                "rf_model = RandomForestModel(random_state=RANDOM_STATE)\n",
                "\n",
                "rf_model.train(\n",
                "    X_train.values, \n",
                "    y_train, \n",
                "    feature_names=feature_names,\n",
                "    tune_hyperparams=True,\n",
                "    cv_folds=5,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "# Evaluate on test set\n",
                "rf_metrics = rf_model.evaluate(X_test.values, y_test, dataset_name=\"Test\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.1 Visualize Random Forest Performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Actual vs Predicted\n",
                "y_pred_rf = rf_model.predict(X_test.values)\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "# Plot 1: Actual vs Predicted\n",
                "plot_actual_vs_predicted(y_test, y_pred_rf, rf_model.name, ax=axes[0])\n",
                "\n",
                "# Plot 2: Residuals\n",
                "plot_residuals(y_test, y_pred_rf, rf_model.name, ax=axes[1])\n",
                "\n",
                "# Plot 3: Residual Distribution\n",
                "plot_residual_distribution(y_test, y_pred_rf, rf_model.name, ax=axes[2])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 Feature Importance (Gini Importance)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature importance\n",
                "rf_importance = rf_model.get_feature_importance()\n",
                "\n",
                "print(\"\\nTop 20 Most Important Features (Random Forest):\")\n",
                "print(rf_importance.head(20).to_string(index=False))\n",
                "\n",
                "# Plot feature importance\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "plot_feature_importance(rf_importance, top_n=20, model_name=rf_model.name, ax=ax)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Model 3: XGBoost Regressor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize and train XGBoost model\n",
                "xgb_model = XGBoostModel(random_state=RANDOM_STATE)\n",
                "\n",
                "xgb_model.train(\n",
                "    X_train.values, \n",
                "    y_train, \n",
                "    feature_names=feature_names,\n",
                "    tune_hyperparams=True,\n",
                "    cv_folds=5,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "# Evaluate on test set\n",
                "xgb_metrics = xgb_model.evaluate(X_test.values, y_test, dataset_name=\"Test\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.1 Visualize XGBoost Performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Actual vs Predicted\n",
                "y_pred_xgb = xgb_model.predict(X_test.values)\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "# Plot 1: Actual vs Predicted\n",
                "plot_actual_vs_predicted(y_test, y_pred_xgb, xgb_model.name, ax=axes[0])\n",
                "\n",
                "# Plot 2: Residuals\n",
                "plot_residuals(y_test, y_pred_xgb, xgb_model.name, ax=axes[1])\n",
                "\n",
                "# Plot 3: Residual Distribution\n",
                "plot_residual_distribution(y_test, y_pred_xgb, xgb_model.name, ax=axes[2])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2 Feature Importance (Gain, Weight, Cover)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature importance (gain)\n",
                "xgb_importance_gain = xgb_model.get_feature_importance(importance_type='gain')\n",
                "\n",
                "print(\"\\nTop 20 Most Important Features (XGBoost - Gain):\")\n",
                "print(xgb_importance_gain.head(20).to_string(index=False))\n",
                "\n",
                "# Plot feature importance\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "plot_feature_importance(xgb_importance_gain, top_n=20, model_name=f\"{xgb_model.name} (Gain)\", ax=ax)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Model Comparison & Best Model Selection"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.1 Comprehensive Comparison Table"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare all models\n",
                "all_models = [best_linear_model, rf_model, xgb_model]\n",
                "final_comparison = compare_models(all_models, X_test.values, y_test)\n",
                "\n",
                "print(\"\\n\" + \"=\"*90)\n",
                "print(\"FINAL MODEL COMPARISON - ALL CLASSICAL MODELS\")\n",
                "print(\"=\"*90)\n",
                "print(final_comparison.to_string(index=False))\n",
                "print(\"=\"*90)\n",
                "\n",
                "# Identify best model\n",
                "best_model_idx = final_comparison['R¬≤'].idxmax()\n",
                "best_model = all_models[best_model_idx]\n",
                "best_model_name = final_comparison.iloc[best_model_idx]['Model']\n",
                "best_r2 = final_comparison.iloc[best_model_idx]['R¬≤']\n",
                "\n",
                "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
                "print(f\"   R¬≤ Score: {best_r2:.4f}\")\n",
                "print(f\"   Best Hyperparameters: {best_model.best_params_}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.2 Visual Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot comparison for different metrics\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "# R¬≤ comparison\n",
                "plot_model_comparison(final_comparison, metric='R¬≤', ax=axes[0, 0])\n",
                "\n",
                "# RMSE comparison\n",
                "plot_model_comparison(final_comparison, metric='RMSE', ax=axes[0, 1])\n",
                "\n",
                "# MAE comparison\n",
                "plot_model_comparison(final_comparison, metric='MAE', ax=axes[1, 0])\n",
                "\n",
                "# Training time comparison\n",
                "plot_model_comparison(final_comparison, metric='Training Time (s)', ax=axes[1, 1])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.3 Side-by-Side Predictions Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create side-by-side actual vs predicted plots\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "for idx, model in enumerate(all_models):\n",
                "    y_pred = model.predict(X_test.values)\n",
                "    plot_actual_vs_predicted(y_test, y_pred, model.name, ax=axes[idx])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.4 Feature Importance Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare top features across models\n",
                "print(\"\\n\" + \"=\"*90)\n",
                "print(\"TOP 10 FEATURES COMPARISON ACROSS MODELS\")\n",
                "print(\"=\"*90)\n",
                "\n",
                "# Create comparison dataframe\n",
                "comparison_data = {\n",
                "    best_linear_model.name: linear_importance.head(10)['feature'].tolist(),\n",
                "    rf_model.name: rf_importance.head(10)['feature'].tolist(),\n",
                "    xgb_model.name: xgb_importance_gain.head(10)['feature'].tolist()\n",
                "}\n",
                "\n",
                "comparison_df = pd.DataFrame(comparison_data)\n",
                "comparison_df.index = range(1, 11)\n",
                "comparison_df.index.name = 'Rank'\n",
                "\n",
                "print(comparison_df.to_string())\n",
                "print(\"=\"*90)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 8. Save Best Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save best model\n",
                "model_save_path = '../data/processed/best_classical_model.pkl'\n",
                "best_model.save(model_save_path)\n",
                "\n",
                "# Save comparison results\n",
                "final_comparison.to_csv('../data/processed/model_comparison_results.csv', index=False)\n",
                "print(\"\\n‚úì Model comparison results saved to: ../data/processed/model_comparison_results.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 9. Summary & Conclusions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*90)\n",
                "print(\"PHASE 2 SUMMARY: CLASSICAL ML MODELS\")\n",
                "print(\"=\"*90)\n",
                "print(f\"\\n‚úì Models Trained: {len(all_models) + 2} (including baseline and linear variants)\")\n",
                "print(f\"‚úì Best Model: {best_model_name}\")\n",
                "print(f\"‚úì Best R¬≤ Score: {best_r2:.4f}\")\n",
                "print(f\"\\nKey Findings:\")\n",
                "print(f\"  1. Best performing model: {best_model_name}\")\n",
                "print(f\"  2. R¬≤ improvement over baseline: {(best_r2 - baseline_metrics['R2']):.4f}\")\n",
                "print(f\"  3. All models successfully trained with hyperparameter tuning\")\n",
                "print(f\"  4. Feature importance analysis completed for all models\")\n",
                "print(f\"\\nNext Steps:\")\n",
                "print(f\"  - Proceed to Phase 3: Literature-based methods reproduction\")\n",
                "print(f\"  - Consider ensemble methods combining best models\")\n",
                "print(f\"  - Extend to multi-subject evaluation (LOSO cross-validation)\")\n",
                "print(\"=\"*90)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}