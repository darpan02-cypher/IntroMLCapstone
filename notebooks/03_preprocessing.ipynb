{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Preprocessing for Mindfulness Prediction\n",
                "\n",
                "This notebook prepares the extracted features for machine learning models.\n",
                "\n",
                "**Steps**:\n",
                "1. Load feature matrix from feature engineering\n",
                "2. Handle missing values (imputation)\n",
                "3. Feature scaling (StandardScaler, MinMaxScaler)\n",
                "4. Train/test split (stratified by label)\n",
                "5. Feature selection (optional)\n",
                "6. Save preprocessed data for modeling\n",
                "\n",
                "**Input**: `data/processed/S2_features.csv`  \n",
                "**Output**: `data/processed/S2_preprocessed.pkl`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "import pickle\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Preprocessing tools\n",
                "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
                "from sklearn.impute import SimpleImputer, KNNImputer\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression\n",
                "\n",
                "# Set plotting style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "print(\"✓ Imports successful\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Feature Matrix\n",
                "\n",
                "Load the features extracted in the previous notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load feature matrix\n",
                "DATA_DIR = Path('../data/processed')\n",
                "subject_id = 'S2'\n",
                "\n",
                "features_path = DATA_DIR / f'{subject_id}_features.csv'\n",
                "\n",
                "if not features_path.exists():\n",
                "    print(f\"ERROR: Feature file not found: {features_path}\")\n",
                "    print(\"Please run 02_feature_engineering.ipynb first to generate features.\")\n",
                "else:\n",
                "    features_df = pd.read_csv(features_path)\n",
                "    print(f\"✓ Loaded feature matrix: {features_df.shape}\")\n",
                "    print(f\"  Rows (windows): {len(features_df)}\")\n",
                "    print(f\"  Columns (features + metadata): {len(features_df.columns)}\")\n",
                "    \n",
                "    # Display first few rows\n",
                "    print(\"\\nFirst 3 rows:\")\n",
                "    display(features_df.head(3))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Separate Features and Target\n",
                "\n",
                "Split the DataFrame into:\n",
                "- **Features (X)**: Physiological features\n",
                "- **Target (y)**: Mindfulness index\n",
                "- **Metadata**: Window indices, labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define metadata columns to exclude from features\n",
                "metadata_cols = ['window_start', 'window_end', 'label', 'mindfulness_index']\n",
                "\n",
                "# Separate features and target\n",
                "X = features_df.drop(columns=metadata_cols)\n",
                "y = features_df['mindfulness_index']\n",
                "metadata = features_df[['window_start', 'window_end', 'label']]\n",
                "\n",
                "print(f\"Features (X): {X.shape}\")\n",
                "print(f\"Target (y): {y.shape}\")\n",
                "print(f\"\\nFeature columns ({len(X.columns)}):\")\n",
                "print(list(X.columns[:10]), \"...\")  # Show first 10\n",
                "\n",
                "# Check target distribution\n",
                "print(f\"\\nTarget (mindfulness_index) distribution:\")\n",
                "print(y.describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Analyze Missing Values\n",
                "\n",
                "Identify features with missing data and decide on imputation strategy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate missing value percentages\n",
                "missing_counts = X.isnull().sum()\n",
                "missing_pct = (missing_counts / len(X)) * 100\n",
                "missing_df = pd.DataFrame({\n",
                "    'Feature': missing_counts.index,\n",
                "    'Missing_Count': missing_counts.values,\n",
                "    'Missing_Pct': missing_pct.values\n",
                "})\n",
                "\n",
                "# Filter to features with missing values\n",
                "missing_features = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Pct', ascending=False)\n",
                "\n",
                "if len(missing_features) > 0:\n",
                "    print(f\"Features with missing values: {len(missing_features)}\")\n",
                "    print(\"\\nTop 10 features by missing percentage:\")\n",
                "    print(missing_features.head(10))\n",
                "    \n",
                "    # Visualize\n",
                "    if len(missing_features) > 0:\n",
                "        plt.figure(figsize=(12, 6))\n",
                "        top_missing = missing_features.head(15)\n",
                "        plt.barh(range(len(top_missing)), top_missing['Missing_Pct'].values)\n",
                "        plt.yticks(range(len(top_missing)), top_missing['Feature'].values)\n",
                "        plt.xlabel('Missing Percentage (%)')\n",
                "        plt.title('Top 15 Features with Missing Values')\n",
                "        plt.gca().invert_yaxis()\n",
                "        plt.tight_layout()\n",
                "        plt.show()\nelse:\n",
                "    print(\"✓ No missing values found!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Handle Missing Values\n",
                "\n",
                "**Strategy**:\n",
                "- Features with >50% missing: Drop\n",
                "- Features with <50% missing: Impute with median (robust to outliers)\n",
                "- Alternative: KNN imputation for better results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define threshold for dropping features\n",
                "DROP_THRESHOLD = 50  # Drop features with >50% missing\n",
                "\n",
                "# Identify features to drop\n",
                "features_to_drop = missing_features[missing_features['Missing_Pct'] > DROP_THRESHOLD]['Feature'].tolist()\n",
                "\n",
                "if len(features_to_drop) > 0:\n",
                "    print(f\"Dropping {len(features_to_drop)} features with >{DROP_THRESHOLD}% missing:\")\n",
                "    print(features_to_drop)\n",
                "    X_cleaned = X.drop(columns=features_to_drop)\n",
                "else:\n",
                "    print(f\"✓ No features exceed {DROP_THRESHOLD}% missing threshold\")\n",
                "    X_cleaned = X.copy()\n",
                "\n",
                "print(f\"\\nFeatures after dropping: {X_cleaned.shape}\")\n",
                "\n",
                "# Impute remaining missing values with median\n",
                "imputer = SimpleImputer(strategy='median')\n",
                "X_imputed = pd.DataFrame(\n",
                "    imputer.fit_transform(X_cleaned),\n",
                "    columns=X_cleaned.columns,\n",
                "    index=X_cleaned.index\n",
                ")\n",
                "\n",
                "# Verify no missing values remain\n",
                "remaining_missing = X_imputed.isnull().sum().sum()\n",
                "print(f\"\\n✓ Imputation complete. Remaining missing values: {remaining_missing}\")\n",
                "print(f\"Final feature matrix: {X_imputed.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Feature Scaling\n",
                "\n",
                "Scale features to have similar ranges for ML algorithms.\n",
                "\n",
                "**Options**:\n",
                "- **StandardScaler**: Mean=0, Std=1 (assumes normal distribution)\n",
                "- **MinMaxScaler**: Scale to [0, 1] range\n",
                "- **RobustScaler**: Uses median and IQR (robust to outliers)\n",
                "\n",
                "We'll use **StandardScaler** as it's most common for regression."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize scaler\n",
                "scaler = StandardScaler()\n",
                "\n",
                "# Fit and transform\n",
                "X_scaled = pd.DataFrame(\n",
                "    scaler.fit_transform(X_imputed),\n",
                "    columns=X_imputed.columns,\n",
                "    index=X_imputed.index\n",
                ")\n",
                "\n",
                "print(f\"✓ Features scaled using StandardScaler\")\n",
                "print(f\"Scaled feature matrix: {X_scaled.shape}\")\n",
                "\n",
                "# Verify scaling\n",
                "print(\"\\nScaling verification (should be ~0 mean, ~1 std):\")\n",
                "print(f\"Mean: {X_scaled.mean().mean():.6f}\")\n",
                "print(f\"Std: {X_scaled.std().mean():.6f}\")\n",
                "\n",
                "# Show before/after for a sample feature\n",
                "sample_feature = X_imputed.columns[0]\n",
                "print(f\"\\nExample feature: {sample_feature}\")\n",
                "print(f\"Before scaling - Mean: {X_imputed[sample_feature].mean():.4f}, Std: {X_imputed[sample_feature].std():.4f}\")\n",
                "print(f\"After scaling  - Mean: {X_scaled[sample_feature].mean():.4f}, Std: {X_scaled[sample_feature].std():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Train/Test Split\n",
                "\n",
                "Split data into training and testing sets.\n",
                "\n",
                "**Strategy**: Stratified split by label to maintain class distribution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set random seed for reproducibility\n",
                "RANDOM_STATE = 42\n",
                "TEST_SIZE = 0.2  # 80% train, 20% test\n",
                "\n",
                "# Stratify by label to maintain distribution\n",
                "stratify_labels = metadata['label']\n",
                "\n",
                "X_train, X_test, y_train, y_test, meta_train, meta_test = train_test_split(\n",
                "    X_scaled, y, metadata,\n",
                "    test_size=TEST_SIZE,\n",
                "    random_state=RANDOM_STATE,\n",
                "    stratify=stratify_labels\n",
                ")\n",
                "\n",
                "print(f\"✓ Train/test split complete\")\n",
                "print(f\"\\nTraining set: {X_train.shape}\")\n",
                "print(f\"Testing set:  {X_test.shape}\")\n",
                "\n",
                "# Verify stratification\n",
                "print(\"\\nLabel distribution:\")\n",
                "print(\"\\nTraining set:\")\n",
                "print(meta_train['label'].value_counts().sort_index())\n",
                "print(\"\\nTesting set:\")\n",
                "print(meta_test['label'].value_counts().sort_index())\n",
                "\n",
                "# Target distribution\n",
                "print(\"\\nTarget (mindfulness_index) distribution:\")\n",
                "print(f\"Train - Mean: {y_train.mean():.3f}, Std: {y_train.std():.3f}\")\n",
                "print(f\"Test  - Mean: {y_test.mean():.3f}, Std: {y_test.std():.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize train/test split\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Target distribution\n",
                "axes[0].hist(y_train, bins=20, alpha=0.7, label='Train', edgecolor='black')\n",
                "axes[0].hist(y_test, bins=20, alpha=0.7, label='Test', edgecolor='black')\n",
                "axes[0].set_xlabel('Mindfulness Index')\n",
                "axes[0].set_ylabel('Frequency')\n",
                "axes[0].set_title('Target Distribution: Train vs Test')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Label distribution\n",
                "train_label_counts = meta_train['label'].value_counts().sort_index()\n",
                "test_label_counts = meta_test['label'].value_counts().sort_index()\n",
                "\n",
                "x = np.arange(len(train_label_counts))\n",
                "width = 0.35\n",
                "\n",
                "axes[1].bar(x - width/2, train_label_counts.values, width, label='Train', alpha=0.8)\n",
                "axes[1].bar(x + width/2, test_label_counts.values, width, label='Test', alpha=0.8)\n",
                "axes[1].set_xlabel('Label')\n",
                "axes[1].set_ylabel('Count')\n",
                "axes[1].set_title('Label Distribution: Train vs Test')\n",
                "axes[1].set_xticks(x)\n",
                "axes[1].set_xticklabels(train_label_counts.index)\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Feature Selection (Optional)\n",
                "\n",
                "Remove low-variance or redundant features to improve model performance.\n",
                "\n",
                "**Methods**:\n",
                "1. **Variance Threshold**: Remove features with low variance\n",
                "2. **Correlation Filter**: Remove highly correlated features\n",
                "3. **SelectKBest**: Select top K features by statistical test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Variance Threshold - Remove features with very low variance\n",
                "variance_threshold = 0.01  # Features with variance < 0.01 are nearly constant\n",
                "\n",
                "selector = VarianceThreshold(threshold=variance_threshold)\n",
                "X_train_var = selector.fit_transform(X_train)\n",
                "X_test_var = selector.transform(X_test)\n",
                "\n",
                "# Get selected feature names\n",
                "selected_features = X_train.columns[selector.get_support()].tolist()\n",
                "removed_features = X_train.columns[~selector.get_support()].tolist()\n",
                "\n",
                "print(f\"Variance Threshold ({variance_threshold}):\")\n",
                "print(f\"  Features kept: {len(selected_features)}\")\n",
                "print(f\"  Features removed: {len(removed_features)}\")\n",
                "\n",
                "if len(removed_features) > 0:\n",
                "    print(f\"\\nRemoved features: {removed_features[:5]}...\")  # Show first 5\n",
                "\n",
                "# Convert back to DataFrame\n",
                "X_train_selected = pd.DataFrame(X_train_var, columns=selected_features, index=X_train.index)\n",
                "X_test_selected = pd.DataFrame(X_test_var, columns=selected_features, index=X_test.index)\n",
                "\n",
                "print(f\"\\nFinal feature matrix after selection:\")\n",
                "print(f\"  Train: {X_train_selected.shape}\")\n",
                "print(f\"  Test: {X_test_selected.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Preprocessed Data\n",
                "\n",
                "Save all preprocessed data for use in modeling notebooks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Package all preprocessed data\n",
                "preprocessed_data = {\n",
                "    'X_train': X_train_selected,\n",
                "    'X_test': X_test_selected,\n",
                "    'y_train': y_train,\n",
                "    'y_test': y_test,\n",
                "    'meta_train': meta_train,\n",
                "    'meta_test': meta_test,\n",
                "    'feature_names': selected_features,\n",
                "    'scaler': scaler,\n",
                "    'imputer': imputer,\n",
                "    'variance_selector': selector,\n",
                "    'metadata': {\n",
                "        'subject_id': subject_id,\n",
                "        'n_features_original': len(X.columns),\n",
                "        'n_features_final': len(selected_features),\n",
                "        'n_train': len(X_train),\n",
                "        'n_test': len(X_test),\n",
                "        'test_size': TEST_SIZE,\n",
                "        'random_state': RANDOM_STATE,\n",
                "        'scaling_method': 'StandardScaler',\n",
                "        'imputation_strategy': 'median'\n",
                "    }\n",
                "}\n",
                "\n",
                "# Save to pickle file\n",
                "output_path = DATA_DIR / f'{subject_id}_preprocessed.pkl'\n",
                "with open(output_path, 'wb') as f:\n",
                "    pickle.dump(preprocessed_data, f)\n",
                "\n",
                "print(f\"✓ Preprocessed data saved to: {output_path}\")\n",
                "\n",
                "# Also save as CSV for easy inspection\n",
                "X_train_selected.to_csv(DATA_DIR / f'{subject_id}_X_train.csv', index=False)\n",
                "X_test_selected.to_csv(DATA_DIR / f'{subject_id}_X_test.csv', index=False)\n",
                "y_train.to_csv(DATA_DIR / f'{subject_id}_y_train.csv', index=False, header=['mindfulness_index'])\n",
                "y_test.to_csv(DATA_DIR / f'{subject_id}_y_test.csv', index=False, header=['mindfulness_index'])\n",
                "\n",
                "print(f\"✓ CSV files saved for inspection\")\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"Preprocessing Complete!\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"Subject: {subject_id}\")\n",
                "print(f\"Original features: {len(X.columns)}\")\n",
                "print(f\"Final features: {len(selected_features)}\")\n",
                "print(f\"Training samples: {len(X_train)}\")\n",
                "print(f\"Testing samples: {len(X_test)}\")\n",
                "print(f\"\\nReady for modeling!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "**Preprocessing Steps Completed**:\n",
                "1. ✓ Loaded feature matrix\n",
                "2. ✓ Handled missing values (dropped >50% missing, imputed rest with median)\n",
                "3. ✓ Scaled features (StandardScaler)\n",
                "4. ✓ Train/test split (80/20, stratified by label)\n",
                "5. ✓ Feature selection (variance threshold)\n",
                "6. ✓ Saved preprocessed data\n",
                "\n",
                "**Next Steps**:\n",
                "- **Notebook 04**: Linear Regression model\n",
                "- **Notebook 05**: Random Forest & XGBoost models\n",
                "- **Notebooks 06-07**: Literature-based methods\n",
                "\n",
                "**Files Created**:\n",
                "- `S2_preprocessed.pkl` - Complete preprocessed dataset\n",
                "- `S2_X_train.csv`, `S2_X_test.csv` - Feature matrices\n",
                "- `S2_y_train.csv`, `S2_y_test.csv` - Target variables"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}